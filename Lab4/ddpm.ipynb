{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eed47e6b",
      "metadata": {},
      "source": [
        "Рустам Шамсутдинов БВТ2201"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2ee411f8",
      "metadata": {
        "id": "2ee411f8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "65845a34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65845a34",
        "outputId": "b89d2a00-7b17-44e2-fcdc-489bde2f8568"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7df5432c57b0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 128\n",
        "lr = 2e-4\n",
        "epochs = 50\n",
        "img_size = 28\n",
        "channels = 1\n",
        "T = 200   # число шагов диффузии\n",
        "beta_start = 1e-4\n",
        "beta_end = 0.02\n",
        "save_dir = \"./ddpm_out\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "80c36888",
      "metadata": {
        "id": "80c36888"
      },
      "outputs": [],
      "source": [
        "betas = torch.linspace(beta_start, beta_end, T, dtype=torch.float32, device=device)\n",
        "alphas = 1.0 - betas\n",
        "alpha_cumprod = torch.cumprod(alphas, dim=0)\n",
        "alpha_cumprod_prev = torch.cat([torch.tensor([1.], device=device), alpha_cumprod[:-1]])\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "sqrt_alpha_cumprod = torch.sqrt(alpha_cumprod)\n",
        "sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - alpha_cumprod)\n",
        "posterior_variance = betas * (1. - alpha_cumprod_prev) / (1. - alpha_cumprod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "357cfcb6",
      "metadata": {
        "id": "357cfcb6"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_ds = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9c11fd7e",
      "metadata": {
        "id": "9c11fd7e"
      },
      "outputs": [],
      "source": [
        "def extract(a, t, x_shape):\n",
        "\n",
        "    batch_size = t.shape[0]\n",
        "    out = a[t].reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
        "    return out\n",
        "\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "    sqrt_alpha_cumprod_t = extract(sqrt_alpha_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alpha_cumprod_t = extract(sqrt_one_minus_alpha_cumprod, t, x_start.shape)\n",
        "    return sqrt_alpha_cumprod_t * x_start + sqrt_one_minus_alpha_cumprod_t * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d560799d",
      "metadata": {
        "id": "d560799d"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, t):\n",
        "        device = t.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = torch.exp(-math.log(10000) * torch.arange(0, half_dim, device=device) / half_dim)\n",
        "        emb = t[:, None] * emb[None, :]\n",
        "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "        return emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "07097b29",
      "metadata": {
        "id": "07097b29"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
        "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        h = self.conv1(x)\n",
        "        h = self.norm1(h)\n",
        "        h = F.silu(h)\n",
        "        time_emb = self.time_mlp(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
        "        h = h + time_emb\n",
        "        h = self.conv2(h)\n",
        "        h = self.norm2(h)\n",
        "        h = F.silu(h)\n",
        "        return h\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, in_ch=1, base_ch=64, time_emb_dim=128):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPosEmb(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "        )\n",
        "        # encoder\n",
        "        self.down1 = Block(in_ch, base_ch, time_emb_dim)\n",
        "        self.down2 = Block(base_ch, base_ch*2, time_emb_dim)\n",
        "        self.down3 = Block(base_ch*2, base_ch*4, time_emb_dim)\n",
        "        # decoder\n",
        "        self.up1 = Block(base_ch*4 + base_ch*2, base_ch*2, time_emb_dim)\n",
        "        self.up2 = Block(base_ch*2 + base_ch, base_ch, time_emb_dim)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(base_ch, base_ch, 3, padding=1),\n",
        "            nn.GroupNorm(8, base_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(base_ch, in_ch, 1),\n",
        "        )\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t = t.float()\n",
        "        t_emb = self.time_mlp(t)\n",
        "        h1 = self.down1(x, t_emb)\n",
        "        h2 = self.down2(self.pool(h1), t_emb)\n",
        "        h3 = self.down3(self.pool(h2), t_emb)\n",
        "        u = self.upsample(h3)\n",
        "        u = torch.cat([u, h2], dim=1)\n",
        "        u = self.up1(u, t_emb)\n",
        "        u = self.upsample(u)\n",
        "        u = torch.cat([u, h1], dim=1)\n",
        "        u = self.up2(u, t_emb)\n",
        "        out = self.final_conv(u)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9df21a12",
      "metadata": {
        "id": "9df21a12"
      },
      "outputs": [],
      "source": [
        "model = SimpleUNet(in_ch=channels, base_ch=32, time_emb_dim=128).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model, beta=0.9999):\n",
        "        self.beta = beta\n",
        "        self.model_shadow = self._clone_model(model)\n",
        "    def _clone_model(self, model):\n",
        "        shadow = type(model)(in_ch=channels, base_ch=32, time_emb_dim=128).to(device)\n",
        "        shadow.load_state_dict(model.state_dict())\n",
        "        for p in shadow.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        return shadow\n",
        "    def update(self, model):\n",
        "        ms = model.state_dict()\n",
        "        for k, v in self.model_shadow.state_dict().items():\n",
        "            v.copy_(v * self.beta + (1. - self.beta) * ms[k].to(v.dtype))\n",
        "    def to(self, device):\n",
        "        self.model_shadow.to(device)\n",
        "\n",
        "ema = EMA(model)\n",
        "\n",
        "mse = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fee88d6",
      "metadata": {
        "id": "8fee88d6"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(x_t, t):\n",
        "    B = x_t.shape[0]\n",
        "    t_tensor = torch.full((B,), t, device=device, dtype=torch.long)\n",
        "    eps_theta = model(x_t, t_tensor)\n",
        "    pred_x0 = (x_t - extract(sqrt_one_minus_alpha_cumprod, t_tensor, x_t.shape) * eps_theta) / extract(sqrt_alpha_cumprod, t_tensor, x_t.shape)\n",
        "    coef1 = extract(betas, t_tensor, x_t.shape) * torch.sqrt(alpha_cumprod_prev[t]) / (1.0 - alpha_cumprod[t])\n",
        "    coef2 = (1.0 - alpha_cumprod_prev[t]) * torch.sqrt(alphas[t]) / (1.0 - alpha_cumprod[t])\n",
        "    posterior_mean = coef1 * pred_x0 + coef2 * x_t\n",
        "    posterior_var = posterior_variance[t]\n",
        "    if t > 0:\n",
        "        noise = torch.randn_like(x_t)\n",
        "        return posterior_mean + torch.sqrt(posterior_var) * noise\n",
        "    else:\n",
        "        return posterior_mean\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(n_samples=64, use_ema=False):\n",
        "    model.eval()\n",
        "    if use_ema:\n",
        "        model_shadow = ema.model_shadow\n",
        "    else:\n",
        "        model_shadow = model\n",
        "    x = torch.randn(n_samples, channels, img_size, img_size, device=device) \n",
        "    for t in reversed(range(T)):\n",
        "        B = x.size(0)\n",
        "        tt = torch.full((B,), t, device=device, dtype=torch.long)\n",
        "        eps_theta = model_shadow(x, tt)\n",
        "        posterior_mean = (1.0 / torch.sqrt(alphas[t])) * (x - (betas[t] / torch.sqrt(1.0 - alpha_cumprod[t])) * eps_theta)\n",
        "        if t > 0:\n",
        "            var = posterior_variance[t]\n",
        "            noise = torch.randn_like(x)\n",
        "            x = posterior_mean + torch.sqrt(var) * noise\n",
        "        else:\n",
        "            x = posterior_mean\n",
        "    model.train()\n",
        "    return x\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_and_save(epoch, n_samples=64, use_ema=True):\n",
        "    imgs = sample(n_samples=n_samples, use_ema=use_ema)\n",
        "    imgs = (imgs + 1.0) / 2.0\n",
        "    imgs = imgs.clamp(0., 1.)\n",
        "    grid = utils.make_grid(imgs.cpu(), nrow=int(math.sqrt(n_samples)), padding=2)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(grid.permute(1,2,0).squeeze(), cmap='gray')\n",
        "    out_path = os.path.join(save_dir, f\"ddpm_samples_epoch_{epoch}.png\")\n",
        "    plt.savefig(out_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Saved samples to {out_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2931a475",
      "metadata": {
        "id": "2931a475"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    global_step = 0\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        for xb, _ in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            b = xb.size(0)\n",
        "            t = torch.randint(0, T, (b,), device=device, dtype=torch.long)\n",
        "            noise = torch.randn_like(xb)\n",
        "            x_noisy = q_sample(xb, t, noise)\n",
        "            predicted_noise = model(x_noisy, t)\n",
        "            loss = mse(predicted_noise, noise)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            ema.update(model)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "            if global_step % 200 == 0:\n",
        "                avg = running_loss / 200\n",
        "                print(f\"Epoch {epoch} step {global_step} avg_loss={avg:.6f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "        sample_and_save(epoch, use_ema=True)\n",
        "    print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ddd44912",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd44912",
        "outputId": "11841230-1027-4f1e-866a-4782a8e66571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 step 200 avg_loss=0.219642\n",
            "Epoch 1 step 400 avg_loss=0.091356\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_1.png\n",
            "Epoch 2 step 600 avg_loss=0.049638\n",
            "Epoch 2 step 800 avg_loss=0.071786\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_2.png\n",
            "Epoch 3 step 1000 avg_loss=0.020613\n",
            "Epoch 3 step 1200 avg_loss=0.064662\n",
            "Epoch 3 step 1400 avg_loss=0.062236\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_3.png\n",
            "Epoch 4 step 1600 avg_loss=0.059030\n",
            "Epoch 4 step 1800 avg_loss=0.059692\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_4.png\n",
            "Epoch 5 step 2000 avg_loss=0.036793\n",
            "Epoch 5 step 2200 avg_loss=0.057546\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_5.png\n",
            "Epoch 6 step 2400 avg_loss=0.015870\n",
            "Epoch 6 step 2600 avg_loss=0.056220\n",
            "Epoch 6 step 2800 avg_loss=0.055970\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_6.png\n",
            "Epoch 7 step 3000 avg_loss=0.051695\n",
            "Epoch 7 step 3200 avg_loss=0.054349\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_7.png\n",
            "Epoch 8 step 3400 avg_loss=0.031977\n",
            "Epoch 8 step 3600 avg_loss=0.053767\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_8.png\n",
            "Epoch 9 step 3800 avg_loss=0.012573\n",
            "Epoch 9 step 4000 avg_loss=0.053518\n",
            "Epoch 9 step 4200 avg_loss=0.052266\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_9.png\n",
            "Epoch 10 step 4400 avg_loss=0.046825\n",
            "Epoch 10 step 4600 avg_loss=0.051980\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_10.png\n",
            "Epoch 11 step 4800 avg_loss=0.028183\n",
            "Epoch 11 step 5000 avg_loss=0.050987\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_11.png\n",
            "Epoch 12 step 5200 avg_loss=0.010463\n",
            "Epoch 12 step 5400 avg_loss=0.050516\n",
            "Epoch 12 step 5600 avg_loss=0.051038\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_12.png\n",
            "Epoch 13 step 5800 avg_loss=0.043395\n",
            "Epoch 13 step 6000 avg_loss=0.050288\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_13.png\n",
            "Epoch 14 step 6200 avg_loss=0.025981\n",
            "Epoch 14 step 6400 avg_loss=0.049809\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_14.png\n",
            "Epoch 15 step 6600 avg_loss=0.008349\n",
            "Epoch 15 step 6800 avg_loss=0.049444\n",
            "Epoch 15 step 7000 avg_loss=0.049508\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_15.png\n",
            "Epoch 16 step 7200 avg_loss=0.040575\n",
            "Epoch 16 step 7400 avg_loss=0.049516\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_16.png\n",
            "Epoch 17 step 7600 avg_loss=0.023557\n",
            "Epoch 17 step 7800 avg_loss=0.049226\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_17.png\n",
            "Epoch 18 step 8000 avg_loss=0.006636\n",
            "Epoch 18 step 8200 avg_loss=0.049498\n",
            "Epoch 18 step 8400 avg_loss=0.048323\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_18.png\n",
            "Epoch 19 step 8600 avg_loss=0.037953\n",
            "Epoch 19 step 8800 avg_loss=0.048913\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_19.png\n",
            "Epoch 20 step 9000 avg_loss=0.021626\n",
            "Epoch 20 step 9200 avg_loss=0.048058\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_20.png\n",
            "Epoch 21 step 9400 avg_loss=0.004815\n",
            "Epoch 21 step 9600 avg_loss=0.048603\n",
            "Epoch 21 step 9800 avg_loss=0.048322\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_21.png\n",
            "Epoch 22 step 10000 avg_loss=0.035765\n",
            "Epoch 22 step 10200 avg_loss=0.047993\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_22.png\n",
            "Epoch 23 step 10400 avg_loss=0.019827\n",
            "Epoch 23 step 10600 avg_loss=0.047434\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_23.png\n",
            "Epoch 24 step 10800 avg_loss=0.003009\n",
            "Epoch 24 step 11000 avg_loss=0.047589\n",
            "Epoch 24 step 11200 avg_loss=0.048219\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_24.png\n",
            "Epoch 25 step 11400 avg_loss=0.034353\n",
            "Epoch 25 step 11600 avg_loss=0.047336\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_25.png\n",
            "Epoch 26 step 11800 avg_loss=0.017573\n",
            "Epoch 26 step 12000 avg_loss=0.047391\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_26.png\n",
            "Epoch 27 step 12200 avg_loss=0.001448\n",
            "Epoch 27 step 12400 avg_loss=0.046918\n",
            "Epoch 27 step 12600 avg_loss=0.047275\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_27.png\n",
            "Epoch 28 step 12800 avg_loss=0.032412\n",
            "Epoch 28 step 13000 avg_loss=0.046880\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_28.png\n",
            "Epoch 29 step 13200 avg_loss=0.015770\n",
            "Epoch 29 step 13400 avg_loss=0.047139\n",
            "Epoch 29 step 13600 avg_loss=0.046788\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_29.png\n",
            "Epoch 30 step 13800 avg_loss=0.046279\n",
            "Epoch 30 step 14000 avg_loss=0.046741\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_30.png\n",
            "Epoch 31 step 14200 avg_loss=0.030238\n",
            "Epoch 31 step 14400 avg_loss=0.046376\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_31.png\n",
            "Epoch 32 step 14600 avg_loss=0.014127\n",
            "Epoch 32 step 14800 avg_loss=0.046600\n",
            "Epoch 32 step 15000 avg_loss=0.046368\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_32.png\n",
            "Epoch 33 step 15200 avg_loss=0.044628\n",
            "Epoch 33 step 15400 avg_loss=0.046819\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_33.png\n",
            "Epoch 34 step 15600 avg_loss=0.028366\n",
            "Epoch 34 step 15800 avg_loss=0.046119\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_34.png\n",
            "Epoch 35 step 16000 avg_loss=0.012576\n",
            "Epoch 35 step 16200 avg_loss=0.046610\n",
            "Epoch 35 step 16400 avg_loss=0.046160\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_35.png\n",
            "Epoch 36 step 16600 avg_loss=0.042938\n",
            "Epoch 36 step 16800 avg_loss=0.046447\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_36.png\n",
            "Epoch 37 step 17000 avg_loss=0.026615\n",
            "Epoch 37 step 17200 avg_loss=0.046041\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_37.png\n",
            "Epoch 38 step 17400 avg_loss=0.010800\n",
            "Epoch 38 step 17600 avg_loss=0.046173\n",
            "Epoch 38 step 17800 avg_loss=0.046097\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_38.png\n",
            "Epoch 39 step 18000 avg_loss=0.040577\n",
            "Epoch 39 step 18200 avg_loss=0.046206\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_39.png\n",
            "Epoch 40 step 18400 avg_loss=0.024817\n",
            "Epoch 40 step 18600 avg_loss=0.045979\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_40.png\n",
            "Epoch 41 step 18800 avg_loss=0.009414\n",
            "Epoch 41 step 19000 avg_loss=0.045548\n",
            "Epoch 41 step 19200 avg_loss=0.045588\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_41.png\n",
            "Epoch 42 step 19400 avg_loss=0.038997\n",
            "Epoch 42 step 19600 avg_loss=0.046056\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_42.png\n",
            "Epoch 43 step 19800 avg_loss=0.023267\n",
            "Epoch 43 step 20000 avg_loss=0.045377\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_43.png\n",
            "Epoch 44 step 20200 avg_loss=0.007558\n",
            "Epoch 44 step 20400 avg_loss=0.045884\n",
            "Epoch 44 step 20600 avg_loss=0.045194\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_44.png\n",
            "Epoch 45 step 20800 avg_loss=0.037113\n",
            "Epoch 45 step 21000 avg_loss=0.045309\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_45.png\n",
            "Epoch 46 step 21200 avg_loss=0.021624\n",
            "Epoch 46 step 21400 avg_loss=0.045800\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_46.png\n",
            "Epoch 47 step 21600 avg_loss=0.006046\n",
            "Epoch 47 step 21800 avg_loss=0.045398\n",
            "Epoch 47 step 22000 avg_loss=0.045372\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_47.png\n",
            "Epoch 48 step 22200 avg_loss=0.035502\n",
            "Epoch 48 step 22400 avg_loss=0.045087\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_48.png\n",
            "Epoch 49 step 22600 avg_loss=0.020167\n",
            "Epoch 49 step 22800 avg_loss=0.045520\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_49.png\n",
            "Epoch 50 step 23000 avg_loss=0.004260\n",
            "Epoch 50 step 23200 avg_loss=0.045501\n",
            "Epoch 50 step 23400 avg_loss=0.044820\n",
            "Saved samples to ./ddpm_out/ddpm_samples_epoch_50.png\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
